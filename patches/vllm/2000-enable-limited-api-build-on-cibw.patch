--- a/setup.py
+++ b/setup.py
@@ -14,6 +14,7 @@ from packaging.version import Version, parse
 from setuptools import Extension, find_packages, setup
 from setuptools.command.build_ext import build_ext
 from torch.utils.cpp_extension import CUDA_HOME
+from wheel.bdist_wheel import bdist_wheel
 
 
 def load_module_from_path(module_name, path):
@@ -234,6 +235,18 @@ class cmake_build_ext(build_ext):
         subprocess.check_call(["cmake", *build_args], cwd=self.build_temp)
 
 
+class bdist_wheel_abi3(bdist_wheel):
+
+    def get_tag(self):
+        python, abi, plat = super().get_tag()
+
+        if python.startswith("cp"):
+            # on CPython, our wheels are abi3 and compatible back to 3.8
+            return "cp38", "abi3", plat
+
+        return python, abi, plat
+
+
 def _is_cuda() -> bool:
     has_cuda = torch.version.cuda is not None
     return (VLLM_TARGET_DEVICE == "cuda" and has_cuda
@@ -440,6 +453,8 @@ def get_requirements() -> List[str]:
 
 ext_modules = []
 
+cmdclass = {"bdist_wheel": bdist_wheel_abi3}
+
 if _is_cuda() or _is_hip():
     ext_modules.append(CMakeExtension(name="vllm._moe_C"))
 
@@ -449,6 +464,8 @@ if _build_custom_ops():
     if _install_punica():
         ext_modules.append(CMakeExtension(name="vllm._punica_C"))
 
+    cmdclass["build_ext"] = cmake_build_ext
+
 package_data = {
     "vllm": ["py.typed", "model_executor/layers/fused_moe/configs/*.json"]
 }
@@ -486,7 +503,7 @@ setup(
     extras_require={
         "tensorizer": ["tensorizer>=2.9.0"],
     },
-    cmdclass={"build_ext": cmake_build_ext} if _build_custom_ops() else {},
+    cmdclass=cmdclass,
     package_data=package_data,
     entry_points={
         "console_scripts": [
